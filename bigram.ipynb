{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "abcdefghijklmnopqrstuvwxyz\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'^': 0, '\\n': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "char_to_token = {\"^\": 0}\n",
    "char_to_token.update({char: i + 1 for i, char in enumerate(chars)})\n",
    "token_to_char = {token:char for char,token in char_to_token.items()}\n",
    "print(char_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(name: str) -> list[int]:\n",
    "    return [char_to_token[\"^\"]] + [char_to_token[char] for char in name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 32033\n",
      "25626 3203 3204\n"
     ]
    }
   ],
   "source": [
    "with open(\"names.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    names = f.readlines()\n",
    "\n",
    "print(f\"Length of dataset: {len(names)}\")\n",
    "\n",
    "training = names[: int(0.8 * len(names))]\n",
    "validation = names[len(training) : len(training) + int(0.1 * len(names))]\n",
    "testing = names[len(training) + len(validation) :]\n",
    "\n",
    "print(len(training), len(validation), len(testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.ones((len(char_to_token.values()), len(char_to_token.values())))\n",
    "\n",
    "for name in training:\n",
    "    for prev_token, token in zip(encode(name), encode(name)[1:]):\n",
    "        probs[prev_token][token] += 1     \n",
    "\n",
    "for token in range(len(probs)):\n",
    "    total = np.sum(probs[token])\n",
    "    if total > 0:\n",
    "        probs[token] = probs[token]/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "co\n",
      "\n"
     ]
    }
   ],
   "source": [
    "token = char_to_token[\"^\"]\n",
    "\n",
    "name = \"\"\n",
    "\n",
    "while token_to_char[token] != \"\\n\":\n",
    "    next_probs = probs[token]\n",
    "    token = np.random.choice(range(len(next_probs)), p=next_probs)\n",
    "    name += token_to_char[token]\n",
    "\n",
    "print(name)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
